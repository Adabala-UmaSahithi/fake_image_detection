# -*- coding: utf-8 -*-
"""Mini Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tjs056NY1aL_G6T5-f9T6bHHPpjj481A
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import numpy as np
from tensorflow.keras.preprocessing import image
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# Parameters
img_size = (128, 128)  # Adjust based on your dataset
batch_size = 16  # Reduced batch size for potential accuracy improvement


# Data Augmentation and Loading
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    shear_range=0.2
)
validation_datagen = ImageDataGenerator(rescale=1.0/255)
test_datagen = ImageDataGenerator(rescale=1.0/255)

train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/Fake image detection dataset-20241104T095406Z-001/Fake image detection dataset/train',
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    classes=['real', 'fake']
)

test_generator = test_datagen.flow_from_directory(
    '/content/drive/MyDrive/Fake image detection dataset-20241104T095406Z-001/Fake image detection dataset/test',
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    classes=['real', 'fake']
)
validation_generator = validation_datagen.flow_from_directory(
    '/content/drive/MyDrive/Fake image detection dataset-20241104T095406Z-001/Fake image detection dataset/validation',
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    classes=['real', 'fake']
)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model

# Load a pretrained model with weights (without the top layers)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))

# Freeze all layers except the last 4
for layer in base_model.layers[:-4]:
    layer.trainable = False

# Add custom layers
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(1, activation='sigmoid')(x)
x = Flatten()(base_model.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(1, activation='sigmoid')(x)


# Compile the model
model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
# Train the model
early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)

# Train the model with callbacks
model.fit(train_generator, epochs=20, validation_data=validation_generator,
          callbacks=[early_stop, model_checkpoint], class_weight={0: 1.0, 1: 1.5})

import numpy as np
from tensorflow.keras.preprocessing import image

def classify_image(model, img_path):
    img = image.load_img('/content/drive/MyDrive/Fake image detection dataset-20241104T095406Z-001/Fake image detection dataset/test/fake/fake_430 - Copy.jpg', target_size=img_size)
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    prediction = model.predict(img_array)
    return "Real" if prediction[0][0] < 0.5 else "Fake"

# Example usage
print(classify_image(model, 'path_to_image.jpg'))

model.save('real_vs_fake_detector2.keras')

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_overall_confusion_matrix(model, train_generator, validation_generator, test_generator):
    # Helper function to get predictions and true labels from a generator
    def get_predictions_and_labels(generator):
        generator.reset()
        predictions = model.predict(generator, steps=generator.samples // generator.batch_size + 1)
        predicted_classes = (predictions > 0.5).astype(int).flatten()
        true_classes = generator.classes
        return predicted_classes, true_classes

    # Get predictions and true labels for each set
    train_pred, train_true = get_predictions_and_labels(train_generator)
    val_pred, val_true = get_predictions_and_labels(validation_generator)
    test_pred, test_true = get_predictions_and_labels(test_generator)

    # Concatenate predictions and true labels for all sets
    overall_predictions = np.concatenate([train_pred, val_pred, test_pred])
    overall_true_labels = np.concatenate([train_true, val_true, test_true])

    # Class labels
    class_labels = list(train_generator.class_indices.keys())

    # Compute overall confusion matrix
    conf_matrix = confusion_matrix(overall_true_labels, overall_predictions)

    # Display confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Overall Confusion Matrix for Entire Dataset')
    plt.show()

    # Print classification report for precision, recall, and F1-score
    report = classification_report(overall_true_labels, overall_predictions, target_names=class_labels)
    print("Classification Report for Entire Dataset:\n", report)

# Call the function to evaluate on the overall dataset (train, validation, and test sets)
evaluate_overall_confusion_matrix(model, train_generator, validation_generator, test_generator)

# Evaluate the model on the train, validation, and test sets
train_loss, train_accuracy = model.evaluate(train_generator, verbose=1)
validation_loss, validation_accuracy = model.evaluate(validation_generator, verbose=1)
test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)

# Print accuracies for each set
print(f"Train Accuracy: {train_accuracy * 100:.2f}%")
print(f"Validation Accuracy: {validation_accuracy * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Calculate and print overall accuracy as an average
overall_accuracy = (train_accuracy + validation_accuracy + test_accuracy) / 3
print(f"Overall Accuracy: {overall_accuracy * 100:.2f}%")

# Call the function to evaluate with metrics, confusion matrix, and classification report for the test set
#evaluate_model_with_metrics(model, test_generator)